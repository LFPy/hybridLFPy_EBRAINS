{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Potjans & Diesmann microcircuit  2014\n",
    "\n",
    "This example notebook submits a job to a chosen HPC backend of EBRAINS/HBP (**JUSUF**), loads a **Singularity** container with a complete **hybridLFPy** installation and executes a version of the hybridLFPy example file ``example_microcircuit.py`` (https://github.com/INM-6/LFPy/blob/nest3/examples/example_microcircuit.py) in parallel on the backend.\n",
    "\n",
    "For more indepth info on using HPC backends via EBRAINS see https://wiki.ebrains.eu/bin/view/Collabs/using-supercomputers-from-the-collab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pyunicore library\n",
    "!pip install pyunicore --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import pyunicore.client as unicore_client\n",
    "import requests\n",
    "import json\n",
    "from time import sleep, time\n",
    "from pprint import pprint\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection to supercomputer (i.e., JUSUF)\n",
    "tr = unicore_client.Transport(clb_oauth.get_token())\n",
    "r = unicore_client.Registry(tr, unicore_client._HBP_REGISTRY_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid choices for supercomputers are one of the keys in:\n",
    "r.site_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supercomputer = 'JUSUF'\n",
    "try:\n",
    "    site_client = r.site(supercomputer)\n",
    "except KeyError:\n",
    "    # cluster may be dropped from Registry.site_urls for whatever reason\n",
    "    site_client = unicore_client.Client(r.transport, 'https://zam2125.zam.kfa-juelich.de:9112/JUSUF/rest/core')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check connection to supercomputer\n",
    "headers = {}\n",
    "headers[\"Authorization\"] = \"Bearer \" + clb_oauth.get_token()\n",
    "headers['Accept'] = \"application/json\"\n",
    "rs = requests.get(url=site_client.site_url, headers = headers, verify = False)\n",
    "print(\"Status code %s \" % rs.status_code)\n",
    "print(\"Content-type %s \" % rs.headers['Content-Type'])\n",
    "reply = rs.json()\n",
    "# print(json.dumps(reply, indent = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download hybridLFPy example files\n",
    "Prepare simulation files using example files from the main hybridLFPy repository (https://github.com/INM-6/hybridLFPy.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --branch nest3 https://github.com/INM-6/hybridLFPy.git"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!wget -O example_network.py https://raw.githubusercontent.com/LFPy/LFPy/master/examples/example_network/example_network.py\n",
    "!wget -O BallAndStick.hoc https://raw.githubusercontent.com/LFPy/LFPy/master/examples/example_network/BallAndStick.hoc\n",
    "!wget -O BallAndStickTemplate.hoc https://raw.githubusercontent.com/LFPy/LFPy/master/examples/example_network/BallAndStickTemplate.hoc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## to make things more interesting with HPCs, let's increase the network size and adjust synaptic weights by applying a patch:\n",
    "diff = '''240c240\n",
    "< population_sizes = [80, 20]\n",
    "---\n",
    "> population_sizes = [1024, 256]\n",
    "257,260c257,260\n",
    "< weightArguments = [[dict(loc=0.002, scale=0.0002),\n",
    "<                     dict(loc=0.002, scale=0.0002)],\n",
    "<                    [dict(loc=0.02, scale=0.002),\n",
    "<                     dict(loc=0.02, scale=0.002)]]\n",
    "---\n",
    "> weightArguments = [[dict(loc=0.0002, scale=0.00002),\n",
    ">                     dict(loc=0.0002, scale=0.00002)],\n",
    ">                    [dict(loc=0.002, scale=0.0002),\n",
    ">                     dict(loc=0.002, scale=0.0002)]]\n",
    "380c380\n",
    "<             ax.plot(t[t >= 200], g[t >= 200], '|', label=name)\n",
    "---\n",
    ">             ax.plot(t[t >= 200], g[t >= 200], '.', ms=2, label=name)\n",
    "'''\n",
    "\n",
    "with open('patch.diff', 'w') as f:\n",
    "    f.writelines(diff)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# apply patch\n",
    "!patch example_network.py patch.diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare singularity container\n",
    "See https://gitlab.version.fz-juelich.de/bvonstvieth_publications/container_userdoc_tmp for details. \n",
    "\n",
    "This step builds the singularity container. It is optional if the recipe has already been uploaded and built on the system. \n",
    "\n",
    "The procedure may be different on different HPC backends. \n",
    "The container can either way be built from the same recipe: https://raw.githubusercontent.com/LFPy/LFPydebian/main/mpich.Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the below Raw block into Code in order to run:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# build singularity container with LFPy using a Dockerfile. \n",
    "# This step only needs to be done once per user/project (possibly). \n",
    "build_job = {\n",
    "  \"Executable\" : \"\"\"module use $OTHERSTAGES\n",
    "module --force purge\n",
    "module load Stages/2020\n",
    "module load GCC Singularity-Tools\n",
    "\n",
    "#mkdir -p ~/.config/sib\n",
    "#cat > ~/.config/sib/settings.ini <<'EOF'\n",
    "#[config]\n",
    "#url_prefix=https://sbuild-hps.fz-juelich.de/\n",
    "#EOF\n",
    "\n",
    "sib upload ./Dockerfile hybridlfpy\n",
    "sib build  # −−recipe−name hybridlfpy −−blocking\n",
    "sib wait-for-completion\n",
    "\"\"\",\n",
    "    \n",
    "  \"Job type\": \"interactive\",\n",
    "}\n",
    "build_job = site_client.new_job(job_description=build_job, inputs=[])\n",
    "while build_job.is_running():\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare main simulation job\n",
    "This step combines in a single session the following:\n",
    "\n",
    "- download hybridLFPy container\n",
    "- download simulation files from the github\n",
    "- ask for resources (# nodes, # MPI processes, # seconds runtime)\n",
    "- execute simulation\n",
    "- zip simulation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with job specification and define list of input files from this Collab\n",
    "# simulation_job = {\"Job type\": \"interactive\"}\n",
    "simulation_job = {}\n",
    "simulation_inputs = [] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Resources\n",
    "simulation_job['Resources'] = {\n",
    "    \"Queue\": \"batch\",\n",
    "    \"Nodes\": \"1\",\n",
    "    \"CPUsPerNode\": \"256\",\n",
    "    \"CPUs\": \"128\",\n",
    "    \"Runtime\": \"600\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "simulation_job['Resources'] = {\n",
    "    \"Queue\": \"batch\",\n",
    "    \"CPUs\": \"512\",\n",
    "    \"Runtime\": \"1200\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands run on login node before job execution\n",
    "simulation_job[\"User precommand\"] = \"\"\"module use $OTHERSTAGES\n",
    "module --force purge\n",
    "module load Stages/2020\n",
    "module load GCC Singularity-Tools git\n",
    "\n",
    "git clone --branch nest3 https://github.com/INM-6/hybridLFPy.git\n",
    "cd hybridLFPy/examples\n",
    "sib download --recipe-name hybridlfpy\n",
    "\"\"\"\n",
    "simulation_job[\"RunUserPrecommandOnLoginNode\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - set some environment variables\n",
    "# - run the python code using interpreter embedded in container\n",
    "simulation_job[\"Executable\"] = \"\"\"module use $OTHERSTAGES\n",
    "module --force purge\n",
    "module load Stages/2020\n",
    "module load GCC Singularity-Tools\n",
    "unset DISPLAY  # matplotlib may look for a nonexistant display on compute node\n",
    "unset SCRATCH  # we're anyway working on $SCRATCH - write sim output to working folder\n",
    "#export SINGULARITYENV_PATH=/opt/nest/bin:$PATH\n",
    "cd hybridLFPy/examples\n",
    "export SINGULARITYENV_PYTHONPATH=/opt/nest/lib/python3.8/site-packages\n",
    "singularity exec -e hybridlfpy.sif nrnivmodl  # compile NEURON NMODL (.mod) files\n",
    "srun --mpi=pmi2 singularity exec -e hybridlfpy.sif python3 -u example_microcircuit.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands run after job is done\n",
    "simulation_job[\"User postcommand\"] = \"\" \n",
    "simulation_job[\"RunUserPostcommandOnLoginNode\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create job\n",
    "job = site_client.new_job(job_description=simulation_job, inputs=simulation_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait while job is running\n",
    "while job.is_running():\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.working_dir.listdir().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STDERR output (if any)\n",
    "stderr = job.working_dir.stat('stderr')\n",
    "pprint(stderr.raw().readlines()[:5])\n",
    "pprint(stderr.raw().readlines()[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STDOUT output\n",
    "stdout = job.working_dir.stat('stdout')\n",
    "pprint(stdout.raw().readlines()[:10])\n",
    "pprint(stdout.raw().readlines()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download simulation output\n",
    "job.working_dir.stat('hybridLFPy/examples/simulation_output_example_microcircuit.tar'\n",
    "                     ).download('simulation_output_example_microcircuit.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill job, clean up files on the remote\n",
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate simulation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# untar output to folder example_network_output\n",
    "!tar -xf simulation_output_example_microcircuit.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at e.g., the extracellular potential and spike raster plot\n",
    "#IFrame(\"./example_network_output/extracellular_potential.pdf\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IFrame(\"./example_network_output/spike_raster.pdf\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
